from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel



def merge_model():
    base_model_name = "Qwen3-8B"  # åŸå§‹æ¨¡å‹å
    lora_path = "medical/output/qwen3_lora"  # LoRA adapter è¾“å‡ºè·¯å¾„

    # 1. åŠ è½½åŸºç¡€æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map="auto", local_files_only=True)

    # 2. åŠ è½½ LoRA æƒé‡
    model = PeftModel.from_pretrained(model, lora_path)

    # 3. åˆå¹¶ LoRA â†’ åŸå§‹æ¨¡å‹
    model = model.merge_and_unload()  # ğŸ”¥ æ ¸å¿ƒä¸€æ­¥ï¼šLoRA èåˆ

    # 2. ä¿å­˜ä¸ºæ–°çš„ Huggingface æ ¼å¼æƒé‡ï¼ˆåˆå¹¶åï¼‰,ç°åœ¨è¿™ä¸ªæ¨¡å‹å°±ä¸å†ä¾èµ– LoRA Adapterï¼Œä¹Ÿä¸å†ä¾èµ– peft åº“åŠ è½½ï¼Œå¯ä»¥ç‹¬ç«‹ç”¨äºæ¨ç†ã€éƒ¨ç½²åˆ° WebUI æˆ– llama.cpp ä¹‹ç±»ç¯å¢ƒã€‚
    # ä¿å­˜ä¸º safetensorsï¼ˆéƒ¨ç½²æ›´å¿«æ›´å®‰å…¨ï¼‰, model.safetensors
    model.save_pretrained("medical/model/qwen3_8b_merged", safe_serialization=True)
    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
    tokenizer.save_pretrained("medical/model/qwen3_8b_merged")
    
if __name__=="__main__":
    merge_model()